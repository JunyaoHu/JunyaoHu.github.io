

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="胡椒">
  <meta name="keywords" content="">
  <meta name="description" content="课程概述 注：  笔记基于Datawhale 202208的组队学习活动，参考李宏毅机器学习课程，对AI基础知识进行梳理，了解的事物不再做记录，仅作不了解的细节的补充。 李宏毅2021偏向深度学习，相对前沿一点的知识；2017&#x2F;2019等偏向机器学习，经典的知识 李宏毅课程资料：太多了，可以去老师官网、油管也有视频，B站也有很多搬运视频，比如啥都会一点的研究生的（他的是两次课程合并了，我不太习惯，">
<meta property="og:type" content="article">
<meta property="og:title" content="李宏毅机器学习 | Datawhale202208 组队学习">
<meta property="og:url" content="http://example.com/2022/08/14/lhy-ml/index.html">
<meta property="og:site_name" content="胡椒的 Coding Room">
<meta property="og:description" content="课程概述 注：  笔记基于Datawhale 202208的组队学习活动，参考李宏毅机器学习课程，对AI基础知识进行梳理，了解的事物不再做记录，仅作不了解的细节的补充。 李宏毅2021偏向深度学习，相对前沿一点的知识；2017&#x2F;2019等偏向机器学习，经典的知识 李宏毅课程资料：太多了，可以去老师官网、油管也有视频，B站也有很多搬运视频，比如啥都会一点的研究生的（他的是两次课程合并了，我不太习惯，">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2022/08/14/bHUWXMrhzDcx8Cv.png">
<meta property="og:image" content="https://oss.linklearner.com/leeml/chapter1/res/chapter1-50.png">
<meta property="og:image" content="https://s2.loli.net/2022/08/23/rIhjORzb8ZTFUCB.png">
<meta property="og:image" content="https://s2.loli.net/2022/08/23/BtizQbIKyoN1eJO.png">
<meta property="og:image" content="https://s2.loli.net/2022/08/23/aTcij8fKCRFh239.png">
<meta property="og:image" content="https://s2.loli.net/2022/08/23/S69QjpuhEbwAOeC.png">
<meta property="og:image" content="https://s2.loli.net/2022/08/28/bQB7fGdmDRxwzHO.png">
<meta property="og:image" content="https://s2.loli.net/2022/08/29/nQo7ZEK3bv8rJUc.png">
<meta property="og:image" content="https://s2.loli.net/2022/08/29/gdK4mOEhNLkDnfw.png">
<meta property="article:published_time" content="2022-08-14T01:52:37.000Z">
<meta property="article:modified_time" content="2022-08-30T07:57:25.232Z">
<meta property="article:author" content="胡椒">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://s2.loli.net/2022/08/14/bHUWXMrhzDcx8Cv.png">
  
  <title>李宏毅机器学习 | Datawhale202208 组队学习 - 胡椒的 Coding Room</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/idea.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.12","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":17340920,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 60vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>胡椒的 Coding Room</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/CV/">
                <i class="iconfont icon-note"></i>
                CV
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/test.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.1)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="李宏毅机器学习 | Datawhale202208 组队学习">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-08-14 09:52" pubdate>
        2022年8月14日 上午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      7.6k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      24 分钟
    </span>
  

  
  
    
      <!-- 不蒜子统计文章PV -->
      <span id="busuanzi_container_page_pv" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="busuanzi_value_page_pv"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">李宏毅机器学习 | Datawhale202208 组队学习</h1>
            
              <p class="note note-info">
                
                  本文最后更新于：2022年8月30日 下午
                
              </p>
            
            <div class="markdown-body">
              <h1 id="课程概述"><a href="#课程概述" class="headerlink" title="课程概述"></a>课程概述</h1><blockquote>
<p><strong><em>注：</em></strong></p>
<ol>
<li>笔记基于Datawhale 202208的组队学习活动，参考李宏毅机器学习课程，对AI基础知识进行梳理，了解的事物不再做记录，仅作不了解的细节的补充。</li>
<li>李宏毅2021偏向深度学习，相对前沿一点的知识；2017/2019等偏向机器学习，经典的知识</li>
<li>李宏毅课程资料：太多了，可以去老师官网、油管也有视频，B站也有很多搬运视频，比如啥都会一点的研究生的（他的是两次课程合并了，我不太习惯，看自己情况），看Datawhale的分开的两次视频也行，同时<a target="_blank" rel="noopener" href="https://linklearner.com/datawhale-homepage/#/learn/detail/93">Datawhale官网</a>还提供了2017/2019版本的笔记便于快速过知识点。</li>
<li>其他的老师还有公司的资料也挺多的，以后有时间会对比不同课程（如李沐、吴恩达、还有飞桨平台的等等教程进行总结）。比如李沐老师的动手学深度学习有全书电子版网站，可以直接复制latex公式很方便。</li>
</ol>
</blockquote>
<p><img src="https://s2.loli.net/2022/08/14/bHUWXMrhzDcx8Cv.png" srcset="/img/loading.gif" lazyload alt="image-20220814095418109"></p>
<h1 id="第一章-机器学习介绍"><a href="#第一章-机器学习介绍" class="headerlink" title="第一章 机器学习介绍"></a>第一章 机器学习介绍</h1><ul>
<li><p>监督学习中的结构化学习：一般说ML是两大类的问题，regression和classification。其实还应该包括structure learning，举例：在语音辨识里面，机器输入是声音讯号，输出是一个句子。句子是要很多词汇拼凑完成。它是一个有结构性的object。或者是说在机器翻译里面你说一句话，你输入中文希望机器翻成英文，它的输出也是有结构性的。或者你今天要做的是人脸辨识，来给机器看张图片。然后机器要把这些东西标出来，这也是一个structure learning问题。</p>
<blockquote>
<p>个人理解其实狭义ML就指的是分类和回归，主要针对的是结构化数据（表格数据）的处理。ML另一部分就是DL，DL包括上文所说的NLP、CV，其实本质也是转为了vector向量然后进行各种处理。总之认为ML主要是使用数学化的一些方法（随机森林等），DL需要搭建深度的网络结构。</p>
</blockquote>
</li>
</ul>
<p><img src="https://oss.linklearner.com/leeml/chapter1/res/chapter1-50.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<h1 id="第二章-为什么要学习机器学习"><a href="#第二章-为什么要学习机器学习" class="headerlink" title="第二章 为什么要学习机器学习"></a>第二章 为什么要学习机器学习</h1><ul>
<li>这章笑死了，直接朗读文本</li>
<li>“我们知道要训练出厉害的AI，AI训练师功不可没”逗号直接读出来，没字读成mei2而不是mo4，这大概也是需要ML/AI的一个原因hhh</li>
</ul>
<h1 id="第三章-回归"><a href="#第三章-回归" class="headerlink" title="第三章 回归"></a>第三章 回归</h1><ul>
<li><p>构建模型基本步骤</p>
<ul>
<li>step1：模型假设，选择模型框架（线性模型等）</li>
<li>step2：模型评估，如何判断众多模型的好坏（损失函数）</li>
<li>step3：模型优化，如何筛选最优的模型（梯度下降）【应该理解为确定模型后如何让该模型尽早达到最优解，这包括调参（参数选择，优化方式选择）】</li>
</ul>
</li>
<li><p>对于最基本的回归模型而言：</p>
<p>选择线性回归LR为model</p>
<script type="math/tex; mode=display">
{\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b</script><p>选择均方误差MSE为loss</p>
<script type="math/tex; mode=display">
l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.</script><p>注：loss函数指单个样本的预测值和真值的偏差，cost函数指整体样本的预测值和真值的偏差</p>
<script type="math/tex; mode=display">
L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.</script><p>选择梯度下降GD为优化方式</p>
<p>问题：</p>
<ul>
<li>问题1：当前最优（Stuck at local minima）</li>
<li>问题2：等于0（Stuck at saddle point）</li>
<li>问题3：趋近于0（Very slow at the plateau）</li>
</ul>
<p>注：通常使用小批量随机梯度下降mini-batch SGD</p>
<script type="math/tex; mode=display">
(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).</script></li>
<li><p>线性回归解析解</p>
</li>
</ul>
<script type="math/tex; mode=display">
\mathbf{w}^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf{y}.</script><ul>
<li>高阶回归模型过拟合严重，需要权衡</li>
<li>优化：整合模型、增加数据、正则化</li>
</ul>
<h1 id="第五章-误差从哪里来"><a href="#第五章-误差从哪里来" class="headerlink" title="第五章 误差从哪里来"></a>第五章 误差从哪里来</h1><ul>
<li><p>error = bias + variance</p>
</li>
<li><p>死去的概率论开始杀我，样本与总体的均值和方差</p>
</li>
<li>偏差大-欠拟合：重新设计模型</li>
<li>方差大-过拟合：更多的数据（含数据增强）等</li>
<li>偏差和方差之间需要权衡</li>
<li>常见误区：模型训练后直接在A榜提交，A榜分数高模型就好，其实在B榜会很拉（有时候还把A的数据合并到test算平均值什么的，算是数据穿越，应当避免）</li>
<li>考虑：从train训练集spilit出val验证集先进行预测，在此基础上延伸出K折交叉验证Kfold，Kfold也有很多种，比如<a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html?highlight=kfold">StratifiedKFold</a>可以按照label的比例进行分割，选择val上平均最好的模型</li>
</ul>
<h1 id="第六章-梯度下降"><a href="#第六章-梯度下降" class="headerlink" title="第六章 梯度下降"></a>第六章 梯度下降</h1><ul>
<li>偏导</li>
</ul>
<script type="math/tex; mode=display">
\nabla f(\mathbf{x}) = \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_d}\bigg]^\top.</script><ul>
<li>选择合适的学习率 $η&gt;0$ 来生成典型的梯度下降算法：</li>
</ul>
<script type="math/tex; mode=display">
\mathbf{x} \leftarrow \mathbf{x} - \eta \nabla f(\mathbf{x}).</script><ul>
<li><p>自适应方法：随着次数的增加，通过一些因子来减少学习率</p>
</li>
<li><p>batch (Vanilla) gradient descent：每次计算在整个数据集上的梯度</p>
<ul>
<li>学习率 $\eta^{t}=\frac{\eta}{\sqrt{t+1}}$</li>
<li>学习率每次对于每一个参数都是一样的</li>
</ul>
</li>
<li><p>Adagrad 算法（别看成adam了）</p>
<ul>
<li><p>提出背景：常见特征的参数相当迅速地收敛到最佳值，而对于不常见的特征，我们仍缺乏足够的观测以确定其最佳值。 换句话说，学习率要么对于常见特征而言降低太慢，要么对于不常见特征而言降低太快。</p>
</li>
<li><p>解决此问题的一个方法是记录我们看到特定特征的次数，然后将其用作调整学习率。 即我们可以使用大小为 $\eta_i = \frac{\eta_0}{\sqrt{s(i, t) + c}}$ 的学习率，而不是 $\eta = \frac{\eta_0}{\sqrt{t + c}}$。 在这里 $s(i, t)$ 计下了我们截至 $t$ 时观察到功能 $i$ 的次数。 这其实很容易实施且不产生额外损耗。</p>
</li>
<li><p>关键步骤：$\mathrm{w}^{\mathrm{t}+1} \leftarrow \mathrm{w}^{\mathrm{t}}-\frac{\eta^{\mathrm{t}}}{\sigma^{\mathrm{t}}} \mathrm{g}^{\mathrm{t}}$</p>
</li>
<li><p>步骤（按照坐标顺序应用，化简后），$\mathbf{s}_0 = \mathbf{0}$</p>
<script type="math/tex; mode=display">
\begin{split}\begin{aligned}
    \mathbf{g}_t & = \partial_{\mathbf{w}} l(y_t, f(\mathbf{x}_t, \mathbf{w})), \\
    \mathbf{s}_t & = \mathbf{s}_{t-1} + \mathbf{g}_t^2, \\
    \mathbf{w}_t & = \mathbf{w}_{t-1} - \frac{\eta}{\sqrt{\mathbf{s}_t + \epsilon}} \cdot \mathbf{g}_t.
\end{aligned}\end{split}</script></li>
<li><p>最好的步伐应该是：一次微分/二次微分，在尽可能不增加过多运算的情况下模拟二次微分。（如果计算二次微分，在实际情况中可能会增加很多的时间消耗）</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>随机梯度下降SGD</p>
<ul>
<li>此时不需要像之前那样对所有的数据进行处理，只需要计算某一个例子的损失函数，就可以赶紧update 梯度。</li>
</ul>
</li>
<li><p>特征缩放</p>
<ul>
<li>标准化的方法（minmax，正态 ）</li>
</ul>
</li>
<li><p>梯度下降的限制</p>
</li>
</ul>
<p>  容易陷入局部极值。还有可能卡在不是极值，但微分值是0的地方。还有可能实际中只是当微分值小于某一个数值就停下来了，但这里只是比较平缓，并不是极值点。</p>
<h1 id="第十三章-深度学习简介"><a href="#第十三章-深度学习简介" class="headerlink" title="第十三章 深度学习简介"></a>第十三章 深度学习简介</h1><ul>
<li>隐藏层本质：通过隐藏层进行特征转换。把隐藏层通过特征提取来替代原来的特征工程，这样在最后一个隐藏层输出的就是一组新的特征（相当于黑箱操作）而对于输出层，其实是把前面的隐藏层的输出当做输入（经过特征提取得到的一组最好的特征）然后通过一个多分类器（可以是softmax函数）得到最后的输出y。</li>
</ul>
<h1 id="第十四章-反向传播"><a href="#第十四章-反向传播" class="headerlink" title="第十四章 反向传播"></a>第十四章 反向传播</h1><ul>
<li>我们的目标是要求计算$\frac{\partial z}{\partial w}$（Forward pass的部分）和计算$\frac{\partial l}{\partial z}$ ( Backward pass的部分 )，然后把$\frac{\partial z}{\partial w}$和$\frac{\partial l}{\partial z}$相乘，我们就可以得到$\frac{\partial l}{\partial w}$,所有我们就可以得到神经网络中所有的参数，然后用梯度下降就可以不断更新，得到损失最小的函数</li>
<li>反向传播其实就是为了简化求导过程，将递归计算导数转换为递推计算倒数，效率更高</li>
</ul>
<h1 id="Lecture-2-What-to-do-if-my-network-fails-to-train"><a href="#Lecture-2-What-to-do-if-my-network-fails-to-train" class="headerlink" title="Lecture 2: What to do if my network fails to train"></a>Lecture 2: What to do if my network fails to train</h1><h2 id="局部最小值与鞍点"><a href="#局部最小值与鞍点" class="headerlink" title="局部最小值与鞍点"></a>局部最小值与鞍点</h2><ul>
<li>Loacl Minima, Saddle Point</li>
<li>维度较高的情况下大部分遇到的都是鞍点</li>
<li>可以通过hessian矩阵、$\lambda$ 的正负判断点的类型</li>
<li>在Saddle Point的下降方法</li>
</ul>
<p><img src="https://s2.loli.net/2022/08/23/rIhjORzb8ZTFUCB.png" srcset="/img/loading.gif" lazyload alt="image-20220823141323115" style="zoom: 67%;" /></p>
<h2 id="批次和动量"><a href="#批次和动量" class="headerlink" title="批次和动量"></a>批次和动量</h2><ul>
<li><p>小批次和动量帮助避免陷入关键点</p>
</li>
<li><p>每一轮分批次的之前可以shuffle</p>
</li>
<li><p>比较</p>
<p><img src="https://s2.loli.net/2022/08/23/BtizQbIKyoN1eJO.png" srcset="/img/loading.gif" lazyload alt="image.png" style="zoom: 67%;" /></p>
</li>
<li><p>动量</p>
<p>李宏毅版本</p>
<script type="math/tex; mode=display">
\begin{split}\begin{aligned}
\mathbf{m}_t &\leftarrow \lambda \mathbf{m}_{t-1} -  \eta_t \mathbf{g}_{t-1}, \\
\mathbf{\theta}_t &\leftarrow \mathbf{\theta}_{t-1} +  \mathbf{m}_t.
\end{aligned}\end{split}</script></li>
</ul>
<p>  <img src="https://s2.loli.net/2022/08/23/aTcij8fKCRFh239.png" srcset="/img/loading.gif" lazyload alt="image.png" style="zoom:50%;" /></p>
<p>  李沐版本</p>
<script type="math/tex; mode=display">
  \begin{split}\begin{aligned}
  \mathbf{v}_t &\leftarrow \beta \mathbf{v}_{t-1} + \mathbf{g}_{t, t-1}, \\
  \mathbf{x}_t &\leftarrow \mathbf{x}_{t-1} - \eta_t \mathbf{v}_t.
  \end{aligned}\end{split}</script><p>  <img src="https://s2.loli.net/2022/08/23/S69QjpuhEbwAOeC.png" srcset="/img/loading.gif" lazyload alt="image.png"></p>
<h2 id="自动调整学习率"><a href="#自动调整学习率" class="headerlink" title="自动调整学习率"></a>自动调整学习率</h2><ul>
<li><p>AdaGrad (前面已介绍)</p>
<script type="math/tex; mode=display">
\begin{split}\begin{aligned}
    \mathbf{g}_t & = \partial_{\mathbf{w}} l(y_t, f(\mathbf{x}_t, \mathbf{w})), \\
    \mathbf{s}_t & = \mathbf{s}_{t-1} + \mathbf{g}_t^2, \\
    \mathbf{w}_t & = \mathbf{w}_{t-1} - \frac{\eta}{\sqrt{\mathbf{s}_t + \epsilon}} \cdot \mathbf{g}_t.
\end{aligned}\end{split}</script></li>
<li><p>RMSProp</p>
<p>改进：Adagrad算法将梯度 $\mathbf{g}_t$ 的平方累加成状态矢量。因此，由于缺乏规范化，没有约束力，持续增长，几乎上是在算法收敛时呈线性递增。</p>
<script type="math/tex; mode=display">
\begin{split}\begin{aligned}
    \mathbf{s}_t & \leftarrow \gamma \mathbf{s}_{t-1} + (1 - \gamma) \mathbf{g}_t^2, \\
    \mathbf{x}_t & \leftarrow \mathbf{x}_{t-1} - \frac{\eta}{\sqrt{\mathbf{s}_t + \epsilon}} \odot \mathbf{g}_t.
\end{aligned}\end{split}</script></li>
<li><p>Adam</p>
<p>$\beta_1 = 0.9, \beta_2 = 0.999, \mathbf{v}_0 = \mathbf{s}_0 = 0$</p>
<script type="math/tex; mode=display">
\begin{split}\begin{aligned}
    \mathbf{v}_t & \leftarrow \beta_1 \mathbf{v}_{t-1} + (1 - \beta_1) \mathbf{g}_t, \\
    \mathbf{s}_t & \leftarrow \beta_2 \mathbf{s}_{t-1} + (1 - \beta_2) \mathbf{g}_t^2.
\end{aligned}\end{split}</script><script type="math/tex; mode=display">
\hat{\mathbf{v}}_t = \frac{\mathbf{v}_t}{1 - \beta_1^t} \text{ and } \hat{\mathbf{s}}_t = \frac{\mathbf{s}_t}{1 - \beta_2^t}.</script><script type="math/tex; mode=display">
\mathbf{x}_t \leftarrow \mathbf{x}_{t-1} - \mathbf{g}_t' = \mathbf{x}_{t-1} - \frac{\eta \hat{\mathbf{v}}_t}{\sqrt{\hat{\mathbf{s}}_t} + \epsilon}.</script></li>
<li><p>学习率调整</p>
<ul>
<li>衰减</li>
<li>warm up （可能的解释：在开始，simga的统计还不够精确）</li>
</ul>
</li>
</ul>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><ul>
<li>MSE（回归）</li>
<li>交叉熵 Cross-entropy（分类）</li>
</ul>
<h2 id="批次标准化"><a href="#批次标准化" class="headerlink" title="批次标准化"></a>批次标准化</h2><ul>
<li><p>batch normalization</p>
</li>
<li><p>由于在训练过程中，中间层的变化幅度不能过于剧烈，而批量规范化将每一层主动居中，并将它们重新调整为给定的平均值和大小</p>
</li>
<li><p>原因</p>
<ul>
<li>数据预处理的方式通常会对最终结果产生巨大影响，它可以将参数的量级进行统一</li>
<li>对于典型的多层感知机或卷积神经网络，中间层中的变量可能具有更广的变化范围批量规范化的发明者非正式地假设，这些变量分布中的这种偏移可能会阻碍网络的收敛。我们可能会猜想，如果一个层的可变值是另一层的100倍，这可能需要对学习率进行补偿调整。</li>
<li>更深层的网络很复杂，容易过拟合。 这意味着正则化变得更加重要。</li>
</ul>
</li>
<li><p>公式</p>
<script type="math/tex; mode=display">
\mathrm{BN}(\mathbf{x}) = \boldsymbol{\gamma} \odot \frac{\mathbf{x} - \hat{\boldsymbol{\mu}}_\mathcal{B}}{\hat{\boldsymbol{\sigma}}_\mathcal{B}} + \boldsymbol{\beta}.</script><script type="math/tex; mode=display">
\begin{split}\begin{aligned} \hat{\boldsymbol{\mu}}_\mathcal{B} &= \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} \mathbf{x},\\
\hat{\boldsymbol{\sigma}}_\mathcal{B}^2 &= \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} (\mathbf{x} - \hat{\boldsymbol{\mu}}_{\mathcal{B}})^2 + \epsilon.\end{aligned}\end{split}</script></li>
<li><p>ps:优化中的各种噪声源通常会导致更快的训练和较少的过拟合：这种变化似乎是正则化的一种形式。 </p>
</li>
<li><p>争议：在提出批量规范化的论文中，作者解释了其原理：通过减少<em>内部协变量偏移</em>（internal covariate shift）。 据推测，作者所说的“内部协变量转移”类似于上述的投机直觉，即变量值的分布在训练过程中会发生变化。 然而，这种解释有两个问题： 1、这种偏移与严格定义的<em>协变量偏移</em>（covariate shift）非常不同，所以这个名字用词不当。 2、这种解释只提供了一种不明确的直觉，但留下了一个有待后续挖掘的问题：为什么这项技术如此有效？</p>
</li>
<li><p>test的batch normalization：使用移动加权平均迭代计算</p>
</li>
</ul>
<h1 id="第二十一章-卷积神经网络"><a href="#第二十一章-卷积神经网络" class="headerlink" title="第二十一章 卷积神经网络"></a>第二十一章 卷积神经网络</h1><ul>
<li><p>cnn能够有效的原理</p>
<ul>
<li>一个神经元探测的是整的图片的一小部分（卷积）</li>
<li>不同的位置的同一个模式不需要不同类型的神经元（卷积）</li>
<li>下采样之后数据减少了但是对识别基本没有影响（池化）</li>
</ul>
</li>
<li><p>属性参数：通道数、filter大小（长宽）、步长、填充padding</p>
</li>
<li><p>cnn基本架构：（卷积+池化）×n+展平+全连接网络</p>
</li>
<li><p>卷积和全连接的关系：拿掉了一些权重，不是全连接，是一种有规律的dropout，减少了需要的参数数量。如何减小：shared weight，一个filter滚动着进行计算，只需要filter尺寸大小的参数即可。</p>
</li>
<li><p>2021关系图</p>
<p><img src="https://s2.loli.net/2022/08/28/bQB7fGdmDRxwzHO.png" srcset="/img/loading.gif" lazyload alt="image.png" style="zoom:67%;" /></p>
</li>
<li><p>应用</p>
<ul>
<li>图像处理（最多）</li>
<li>围棋：并没有用到pooling，每一个位置都用48个value来描述</li>
<li>语音：时频图，在语音上，我们通常只考虑在frequency方向上移动的filter</li>
<li>文字：nlp相关，word embedding，构建词向量。你把filter沿着句子里面词汇的顺序来移动，然后你就会得到一个vector。在文字处理上，filter只在时间的序列上移动（可以单向也可以双向，和时间有关，加入LSTM），不会在embedding 维度这个方向上移动。</li>
</ul>
</li>
</ul>
<h1 id="DataWhale组队学习小结"><a href="#DataWhale组队学习小结" class="headerlink" title="DataWhale组队学习小结"></a>DataWhale组队学习小结</h1><ul>
<li>关于学习：只学习了李宏毅机器学习的机器学习部分，对于2021新版的话后面深度学习的部分不在这次组队学习的学习范围内，首先是自己对机器学习的知识有了更深入的学习，不限于表面了，加深了对理论方法形成的过程的认知，以及记住了实现的公式。当然学习无止境，接下来自己会继续学习完剩余的章节，主动学习知识。</li>
<li>关于社区：第一次参加组队学习的活动hh，之前参加过一些DataWhale的教程内测活动，感觉挺不错的，是个很对新手很友好的社区，基本上理论、代码、实践竞赛都有涉及，大佬们也是很用心在经营社区，也有很多志同道合的同学在社区相互讨论交流，相互进步，希望DataWhale越来越好 ^ ^</li>
</ul>
<h1 id="Lecture-4-Sequence-as-input"><a href="#Lecture-4-Sequence-as-input" class="headerlink" title="Lecture 4: Sequence as input"></a>Lecture 4: Sequence as input</h1><ul>
<li>one-hot：看不出来距离</li>
<li>word enbbedding：也是向量，但是相似词语距离相近</li>
<li>output：n2n（词性标注），n21（正负情感分析），n2x（句子翻译）</li>
<li>Self-attention：主要是，先考虑了整个句子vec2vec，然后fc，也有变式</li>
</ul>
<ul>
<li>过程<ol>
<li>在原向量列表中确定当前向量相关的向量距离（关联程度）<ul>
<li>dot-product常用</li>
<li>additive</li>
</ul>
</li>
<li>自主性提示称为查询（query）。感官输入被称为值（value）</li>
<li>计算得到当前query和其他key的不同attention score</li>
<li>根据分数抽取重要资讯，attention score和矩阵v相乘得到当前向量b，谁分量大谁关联强</li>
</ol>
</li>
</ul>
<p><img src="https://s2.loli.net/2022/08/29/nQo7ZEK3bv8rJUc.png" srcset="/img/loading.gif" lazyload alt="image.png" style="zoom:50%;" /></p>
<p><img src="https://s2.loli.net/2022/08/29/gdK4mOEhNLkDnfw.png" srcset="/img/loading.gif" lazyload alt="image.png" style="zoom:50%;" /></p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E8%AF%BE%E5%A4%96%E5%AD%A6%E4%B9%A0/">课外学习</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
                    
                      <a class="hover-with-bg" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/08/10/30days-of-ml-202208-sd/">
                        <span class="hidden-mobile">广告-信息流跨域CTR预估 - Coggle 30 Days of ML（22年8月）</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div id="twikoo"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/twikoo@1/dist/twikoo.all.min.js', function() {
        var options = Object.assign(
          {"envId":"https://blog-comment-h866w5ycd-junyaohu.vercel.app/","region":"ap-shanghai","path":"window.location.pathname"},
          {
            el: '#twikoo',
            path: 'window.location.pathname',
            onCommentLoaded: function() {
              Fluid.plugins.initFancyBox('#twikoo .tk-content img:not(.tk-owo-emotion)');
            }
          }
        )
        twikoo.init(options)
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  








  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?17340920";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
